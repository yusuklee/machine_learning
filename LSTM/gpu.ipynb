{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:50.749225700Z",
     "start_time": "2026-01-13T09:03:47.191210400Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from unicodedata import bidirectional\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:50.779428100Z",
     "start_time": "2026-01-13T09:03:50.749225700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "print(device)"
   ],
   "id": "a161fc6ed2d1c5a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:50.797597900Z",
     "start_time": "2026-01-13T09:03:50.779428100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_length = 28 # MNIST row 를 일종의 순서(sequence) 로 다룸\n",
    "feature_size = 28 # 입력 차원\n",
    "hidden_size = 128 # Hidden Layer 사이즈 설정처럼 설정\n",
    "num_layers = 4 # stacked RNN (최대 4개까지는 Gradient Vanishing 현상이 적을 수 있으므로)\n",
    "dropout_p = 0.2 # dropout rate\n",
    "output_size = 10 # 0 ~ 9 숫자 부류(클래스)\n",
    "minibatch_size = 128 # minibatch_size"
   ],
   "id": "7c65f0f632377a3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:50.851303900Z",
     "start_time": "2026-01-13T09:03:50.799602600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = datasets.MNIST(root='dataset',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.ToTensor()\n",
    "                       )\n",
    "\n",
    "test = datasets.MNIST(\n",
    "    root='dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "print('훈련 데이터 길이 : ', len(train))\n",
    "print('테스트 데이터 크기:', len(test))"
   ],
   "id": "630a7493162d8096",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 길이 :  60000\n",
      "테스트 데이터 크기: 10000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:50.919950700Z",
     "start_time": "2026-01-13T09:03:50.851303900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_indices, valid_indices, _, _ = train_test_split(\n",
    "    range(len(train)),\n",
    "    train.targets,\n",
    "    stratify=train.targets,\n",
    "    test_size = 0.2\n",
    ")\n",
    "\n",
    "train = Subset(train, train_indices)\n",
    "valid = Subset(train, valid_indices)\n",
    "\n",
    "minibatch_size = 128\n",
    "train_batches = DataLoader(train, batch_size=minibatch_size, shuffle=True)\n",
    "val_batches = DataLoader(valid, batch_size=minibatch_size, shuffle=True)\n",
    "test_batches = DataLoader(test, batch_size=minibatch_size, shuffle=True)"
   ],
   "id": "ef1c8a8b8b37255",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:50.925091800Z",
     "start_time": "2026-01-13T09:03:50.919950700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "99680ead94c8bfba",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:51.067781100Z",
     "start_time": "2026-01-13T09:03:50.927096100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, dropout_p, output_size, model_type):\n",
    "        super().__init__()\n",
    "        if model_type == 'rnn':\n",
    "            self.go = nn.RNN(\n",
    "               input_size = feature_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = dropout_p,\n",
    "                bidirectional = True\n",
    "            )\n",
    "        elif model_type=='lstm':\n",
    "            self.go = nn.LSTM(\n",
    "                input_size = feature_size,\n",
    "                hidden_size= hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_p,\n",
    "                bidirectional=True\n",
    "            )\n",
    "        self.go2 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm1d(hidden_size*2),\n",
    "            nn.Linear(hidden_size*2, output_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out, _ = self.go(x)\n",
    "        out = out[:, -1]\n",
    "        y= self.go2(out)\n",
    "        return y\n",
    "\n",
    "model = Net(feature_size, hidden_size, num_layers, dropout_p, output_size, 'lstm').to(device)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "id": "db83d01cd673fff7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:03:59.986765500Z",
     "start_time": "2026-01-13T09:03:52.893034800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, early, n_epochs, progress_interval):\n",
    "    train_losses, valid_losses, lowest_loss = [], [], np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss , valid_loss = 0,0\n",
    "\n",
    "        model.train()\n",
    "        for x,y  in train_batches:\n",
    "            x = x.reshape(-1,sequence_length, feature_size).to(device)\n",
    "            y= y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "        train_loss/=len(train_batches)\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_batches:\n",
    "                x = x.reshape(-1,sequence_length, feature_size)\n",
    "                x = x.to(device)\n",
    "                y= y.to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = loss_func(y_pred, y)\n",
    "                valid_loss+=loss.item()\n",
    "\n",
    "        valid_loss = valid_loss/len(val_batches)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        if valid_losses[-1] < lowest_loss:\n",
    "            lowest_loss = valid_losses[-1]\n",
    "            lowest_epoch = epoch\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            if(early>0) and lowest_epoch + early<epoch:\n",
    "                print('early stopped')\n",
    "                break\n",
    "        if (epoch%progress_interval==0):\n",
    "            print(train_losses[-1], valid_loss[-1])\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, lowest_loss, train_losses, valid_losses\n",
    "\n",
    "\n",
    "nb_epochs = 100\n",
    "progress_interval = 3\n",
    "early_stop = 30\n",
    "\n",
    "model, lowest_loss, train_losses, valid_losses = train_model(model, early_stop, nb_epochs, progress_interval)"
   ],
   "id": "ec8ae85c80af7d52",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 53\u001B[39m\n\u001B[32m     50\u001B[39m progress_interval = \u001B[32m3\u001B[39m\n\u001B[32m     51\u001B[39m early_stop = \u001B[32m30\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m model, lowest_loss, train_losses, valid_losses = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_interval\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, early, n_epochs, progress_interval)\u001B[39m\n\u001B[32m     21\u001B[39m model.eval()\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mval_batches\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\yusuk1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\yusuk1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    786\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    787\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m788\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    790\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\yusuk1\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     52\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\yusuk1\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:414\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitems__\u001B[39m(\u001B[38;5;28mself\u001B[39m, indices: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m]) -> \u001B[38;5;28mlist\u001B[39m[_T_co]:\n\u001B[32m    411\u001B[39m     \u001B[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001B[39;00m\n\u001B[32m    412\u001B[39m     \u001B[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001B[39;00m\n\u001B[32m    413\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[32m--> \u001B[39m\u001B[32m414\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    415\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    416\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m.dataset[\u001B[38;5;28mself\u001B[39m.indices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\yusuk1\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    414\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    415\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m416\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m.dataset[\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[31mIndexError\u001B[39m: list index out of range"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d17d27c9c3ac7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
